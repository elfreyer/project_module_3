{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00cd7439",
   "metadata": {},
   "source": [
    "# Cifar10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5796909",
   "metadata": {},
   "source": [
    "# Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d98aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b0a1f7",
   "metadata": {},
   "source": [
    "# Setup\n",
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5175626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "\n",
    "# TODO: investigate the rescalling option\n",
    "# IMG_SIZE = 160 # origninal img size (32, 32, 3), optimal img size (224, 224, 3)\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform: Compose = Compose([\n",
    "    \n",
    "    Resize((224, 224)),\n",
    "    \n",
    "    # Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] \n",
    "    # to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "    ToTensor(), \n",
    "    \n",
    "    # Normalize a tensor image with mean and standard deviation.\n",
    "    # output[channel] = (input[channel] - mean[channel]) / std[channel] -> normalised = (original - 0.5) / 0.5]\n",
    "    # Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be58c021",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ce6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# load the full CIFAR-10 training data\n",
    "full_trainset: CIFAR10 = CIFAR10(\n",
    "    root='../data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# split the full training data into train and validation sets\n",
    "train_size: int = int(0.8 * len(full_trainset))\n",
    "valid_size: int = len(full_trainset) - train_size\n",
    "\n",
    "# TODO: check annotation type, and understand the function\n",
    "trainset, validset = random_split(full_trainset, [train_size, valid_size])\n",
    "\n",
    "# load the test data\n",
    "testset: CIFAR10 = CIFAR10(\n",
    "    root='../data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bd9cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataset sizes\n",
    "print(f\"All samples: {len(trainset)+len(validset)+len(testset)}\")\n",
    "print(f\"Train samples: {len(trainset)}\")\n",
    "print(f\"Validation samples: {len(validset)}\")\n",
    "print(f\"Test samples: {len(testset)}\")\n",
    "\n",
    "# get one sample to check image shape\n",
    "image, label = full_trainset[0]\n",
    "print(f\"Single image shape: {image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f193b28",
   "metadata": {},
   "source": [
    "## Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8741d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader: DataLoader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4,    \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "# \n",
    "# validloader = torch.utils.data.DataLoader(\n",
    "valid_loader: DataLoader = DataLoader(\n",
    "    validset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=4,    \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_loader: DataLoader = DataLoader(\n",
    "    testset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=4,    \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# Check shapes\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"Batch shape:\", xb.shape)   # should be [B, 3, 32, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971a1eca",
   "metadata": {},
   "source": [
    "## View a batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e39f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# TODO: check the bug -> it is not an 8 by 8 figure, why?\n",
    "# Helper function to display an image grid\n",
    "def show_image(image_tensor: torch.Tensor) -> None:\n",
    "    \"\"\"Convert a PyTorch tensor into a viewable image grid and show it.\"\"\"\n",
    "\n",
    "    # Step 1 Undo normalization: normalised = (original - 0.5) / 0.5] -> normalised * 2 +0.5 = original\n",
    "    unnormalized_image_grid = image_tensor * 0.5 + 0.5\n",
    "\n",
    "    # Step 2: Convert from a PyTorch tensor to a NumPy array (for matplotlib)\n",
    "    # .detach() removes gradient tracking; .cpu() ensures data is on the CPU.\n",
    "    image_grid_C_H_W = unnormalized_image_grid.detach().cpu().numpy()\n",
    "    \n",
    "    print(\"Shape before transpose:\", image_grid_C_H_W.shape)  \n",
    "    # (C, H, W) → C = number of color channels (3 for RGB), H = height, W = width\n",
    "\n",
    "    # Step 3: Rearrange dimensions from (C, H, W) to (H, W, C)\n",
    "    # because Matplotlib expects the color channel as the last dimension.\n",
    "    image_grid_H_W_C = np.transpose(image_grid_C_H_W, (1, 2, 0))\n",
    "    print(\"Shape after transpose:\", image_grid_H_W_C.shape)   # (H, W, C)\n",
    "\n",
    "    # Step 4: Display the image\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(image_grid_H_W_C)\n",
    "    plt.axis(\"off\")  # hide axis numbers\n",
    "    plt.show()\n",
    "    \n",
    "# Get a batch of training images\n",
    "data_iter: iter = iter(train_loader)  # Create an iterator for the DataLoader\n",
    "images: torch.Tensor\n",
    "labels: torch.Tensor\n",
    "images, labels = next(data_iter)   # Get one batch (images + labels)\n",
    "\n",
    "# Make a grid from the batch\n",
    "image_grid: torch.Tensor = torchvision.utils.make_grid(images, nrow=8)  # 8 images per row\n",
    "\n",
    "\n",
    "# Show the grid\n",
    "show_image(image_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3da908",
   "metadata": {},
   "source": [
    "## Get class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fcf06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get the label directly cifar10_classes = cifar10.classes\n",
    "\n",
    "cifar10_classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(cifar10_classes [labels[0]])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6889a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7538161a",
   "metadata": {},
   "source": [
    "# Convolutional Variational Autoencoder (Conv-VAE)\n",
    "https://github.com/ageron/handson-mlp/blob/main/18_autoencoders_gans_and_diffusion_models.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a46f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d30d8",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a175176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60463daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: rename VAE model to CVAE\n",
    "# TODO: try batchnorm, dropout\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import namedtuple\n",
    "\n",
    "VAEOutput = namedtuple(\"VAEOutput\", [\"output\", \"codings_mean\", \"codings_logvar\"])\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, codings_dim=32, p_drop=0.2):\n",
    "        \"\"\"\n",
    "        Convolutional VAE for 224x224 images.\n",
    "        Inputs in [0,1]; outputs in [0,1].\n",
    "        p_drop: dropout probability (try 0.1–0.3)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.codings_dim = codings_dim\n",
    "\n",
    "        # --------- Encoder ---------\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            # 224 -> 112\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p_drop),\n",
    "\n",
    "            # 112 -> 56\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p_drop),\n",
    "\n",
    "            # 56 -> 28\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p_drop),\n",
    "\n",
    "            # 28 -> 14\n",
    "            nn.Conv2d(128, 192, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(192), nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p_drop),\n",
    "\n",
    "            # 14 -> 7\n",
    "            nn.Conv2d(192, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "\n",
    "            # keep 7x7 with a 3x3 conv\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.flat_dim = 256 * 7 * 7  # 12544\n",
    "\n",
    "        # --------- Variational heads ---------\n",
    "        self.fc_mu = nn.Sequential(\n",
    "            nn.Linear(self.flat_dim, codings_dim),\n",
    "            nn.Dropout(p_drop)\n",
    "        )\n",
    "        self.fc_logvar = nn.Sequential(\n",
    "            nn.Linear(self.flat_dim, codings_dim),\n",
    "            nn.Dropout(p_drop)\n",
    "        )\n",
    "\n",
    "        # --------- Decoder ---------\n",
    "        self.fc_dec = nn.Linear(codings_dim, self.flat_dim)\n",
    "\n",
    "        self.decoder_cnn = nn.Sequential(\n",
    "            # 7 -> 14\n",
    "            nn.ConvTranspose2d(256, 192, 4, 2, 1),\n",
    "            nn.BatchNorm2d(192), nn.ReLU(inplace=True),\n",
    "\n",
    "            # 14 -> 28\n",
    "            nn.ConvTranspose2d(192, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "\n",
    "            # 28 -> 56\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "\n",
    "            # 56 -> 112\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "\n",
    "            # 112 -> 224\n",
    "            nn.ConvTranspose2d(32, 16, 4, 2, 1),\n",
    "            nn.BatchNorm2d(16), nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    # --------- Core Methods ---------\n",
    "    def encode(self, x):\n",
    "        h = self.encoder_cnn(x).flatten(1)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_codings(mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.fc_dec(z).view(-1, 256, 7, 7)\n",
    "        return self.decoder_cnn(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.sample_codings(mu, logvar)\n",
    "        output = self.decode(z)\n",
    "        return VAEOutput(output, mu, logvar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6e25a",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(y_pred, x_target, kl_weight=1.0):\n",
    "    output, mean, logvar = y_pred\n",
    "    # MSE reconstruction\n",
    "    recon = F.mse_loss(output, x_target)\n",
    "    # recon = F.binary_cross_entropy_with_logits(output, x_target, reduction=\"mean\") # logits + BCEWithLogitsLoss:\n",
    "    # KL divergence\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - logvar.exp() - mean.pow(2), dim=-1).mean()\n",
    "    # Scale KL to per-pixel magnitude\n",
    "    n_pixels = x_target[0].numel()\n",
    "    return recon + kl_weight * (kl_div / n_pixels)\n",
    "\n",
    "def evaluate_tm(model, data_loader, metric):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            out = y_pred.output if isinstance(y_pred, tuple) else y_pred\n",
    "            metric.update(out, X_batch)  # compare recon to input\n",
    "    return metric.compute()\n",
    "\n",
    "def train(model, optimizer, loss_fn, metric, train_loader, valid_loader, n_epochs=20):\n",
    "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.0\n",
    "        metric.reset()\n",
    "        model.train()\n",
    "        for index, (X_batch, _) in enumerate(train_loader):\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, X_batch)   # use input as target\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = y_pred.output if isinstance(y_pred, tuple) else y_pred\n",
    "            metric.update(out, X_batch)\n",
    "\n",
    "        train_metric = metric.compute().item()\n",
    "        val_metric = evaluate_tm(model, valid_loader, metric).item()\n",
    "\n",
    "        history[\"train_losses\"].append(total_loss / len(train_loader))\n",
    "        history[\"train_metrics\"].append(train_metric)\n",
    "        history[\"valid_metrics\"].append(val_metric)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d}/{n_epochs} \"\n",
    "              f\"loss={history['train_losses'][-1]:.4f}, \"\n",
    "              f\"train RMSE={train_metric:.4f}, val RMSE={val_metric:.4f}\")\n",
    "    return history\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "cvae = CVAE(codings_dim=256).to(device)\n",
    "# TODO: hyperparameter to tune -> learning rate\n",
    "optimizer = torch.optim.NAdam(cvae.parameters(), lr=1e-4)\n",
    "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
    "\n",
    "history = train(cvae, optimizer, vae_loss, rmse, train_loader, valid_loader, n_epochs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada83e7e",
   "metadata": {},
   "source": [
    "Note: Evaluate the range for rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d82386",
   "metadata": {},
   "source": [
    "## Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdab9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add val loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Plot training loss ---\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history[\"train_losses\"], label=\"Train loss\", color=\"tab:blue\")\n",
    "plt.title(\"CVAE Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot RMSE (train vs validation) ---\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history[\"train_metrics\"], label=\"Train RMSE\", color=\"tab:green\")\n",
    "plt.plot(history[\"valid_metrics\"], label=\"Validation RMSE\", color=\"tab:orange\")\n",
    "plt.title(\"Reconstruction RMSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f46c9",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927b69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_image(img):\n",
    "    \"\"\"Convert a tensor to a displayable image and plot it.\"\"\"\n",
    "    img = img.detach().cpu()\n",
    "    if img.ndim == 3:\n",
    "        img = img.permute(1, 2, 0)  # (C,H,W) -> (H,W,C)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def plot_reconstructions(model, data_loader, n_images=8):\n",
    "    \"\"\"\n",
    "    Show original (top row) and reconstructed (bottom row) images from the VAE.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # --- Get a batch of images ---\n",
    "    X_batch, _ = next(iter(data_loader))\n",
    "    X_batch = X_batch.to(device)\n",
    "    X_batch = X_batch[:n_images]  # take first n_images only\n",
    "\n",
    "    # --- Forward pass ---\n",
    "    with torch.no_grad():\n",
    "        out = model(X_batch)\n",
    "        recon = out.output if hasattr(out, \"output\") else out\n",
    "\n",
    "    # --- Plot originals and reconstructions ---\n",
    "    fig, axes = plt.subplots(2, n_images, figsize=(n_images * 1.5, 3))\n",
    "\n",
    "    for i in range(n_images):\n",
    "        # Top row: original\n",
    "        axes[0, i].imshow(X_batch[i].detach().cpu().permute(1, 2, 0))\n",
    "        axes[0, i].axis(\"off\")\n",
    "\n",
    "        # Bottom row: reconstruction\n",
    "        axes[1, i].imshow(recon[i].detach().cpu().permute(1, 2, 0))\n",
    "        axes[1, i].axis(\"off\")\n",
    "    \n",
    "\n",
    "    fig.suptitle(\"Top: Original images | Bottom: Reconstructions\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Use it ---\n",
    "plot_reconstructions(cvae, valid_loader, n_images=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee3e58",
   "metadata": {},
   "source": [
    "# Exploration of the latent represenation (umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def collect_mu(vae, loader, device):\n",
    "    vae.eval()\n",
    "    mus, ys = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            out = vae(x)\n",
    "            # Handle both: namedtuple VAEOutput or plain tensor\n",
    "            mu = out.codings_mean if hasattr(out, \"codings_mean\") else vae.encode(x)[0]\n",
    "            mus.append(mu.detach().cpu().numpy())\n",
    "            ys.append(y.numpy())\n",
    "    X_mu = np.concatenate(mus, axis=0)   # shape: (N, codings_dim)\n",
    "    y_all = np.concatenate(ys, axis=0)   # shape: (N,)\n",
    "    return X_mu, y_all\n",
    "\n",
    "# Example:\n",
    "X_train_mu, y_train = collect_mu(cvae, train_loader, device)\n",
    "X_valid_mu, y_valid = collect_mu(cvae, valid_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bee2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install umap-learn\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# TODO: try to improve by playing with the parameters\n",
    "\n",
    "umap_2d = umap.UMAP(\n",
    "    n_neighbors=15,      # local vs global structure (try 10–50)\n",
    "    min_dist=0.1,        # how tight clusters look (0.0–0.5)\n",
    "    n_components=2,      # 2D for plotting (set 3 for 3D)\n",
    "    metric=\"cosine\",  # try \"cosine\" if features are directional\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Z_train_2d = umap_2d.fit_transform(X_train_mu)  # (N_train, 2)\n",
    "# Z_valid_2d = umap_2d.transform(X_valid_mu)      # (N_valid, 2)\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler().fit(X_train_mu)\n",
    "X_train_mu_s = scaler.transform(X_train_mu)\n",
    "X_valid_mu_s = scaler.transform(X_valid_mu)\n",
    "Z_train_2d = umap_2d.fit_transform(X_train_mu_s)\n",
    "Z_valid_2d = umap_2d.transform(X_valid_mu_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a4295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scatter_umap(Z, y, title=\"UMAP of VAE μ\", num_classes=None):\n",
    "    plt.figure(figsize=(7,6))\n",
    "    sc = plt.scatter(Z[:,0], Z[:,1], c=y, s=8, alpha=0.8, cmap=\"tab10\")\n",
    "    if num_classes is None:\n",
    "        num_classes = len(np.unique(y))\n",
    "    plt.colorbar(sc, ticks=range(num_classes))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"UMAP-1\"); plt.ylabel(\"UMAP-2\")\n",
    "    plt.grid(True, ls=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "scatter_umap(Z_train_2d, y_train, title=\"Train μ → UMAP (2D)\")\n",
    "scatter_umap(Z_valid_2d, y_valid, title=\"Valid μ → UMAP (2D)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827f9ecb",
   "metadata": {},
   "source": [
    "Overall structure\n",
    "\n",
    "* The map shows a continuous, dense cloud rather than distinct clusters.\n",
    "\n",
    "* That’s expected — your autoencoder is trained unsupervised only to reconstruct images, not to separate classes.\n",
    "\n",
    "* Therefore, latent codes mainly capture visual similarity (colors, textures, shapes) rather than semantic categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc0ec2b",
   "metadata": {},
   "source": [
    "# Find outliers\n",
    "test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28753ba",
   "metadata": {},
   "source": [
    "# Sample from the latent distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef77f5",
   "metadata": {},
   "source": [
    "# overlay with data points\n",
    "What does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038db47b",
   "metadata": {},
   "source": [
    "# Feature extraction \n",
    "From latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee655b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "device = next(cvae.parameters()).device\n",
    "\n",
    "# --- helper to collect μ and labels from a loader ---\n",
    "def collect_mu(cvae, loader, device, name=\"dataset\"):\n",
    "    cvae.eval()\n",
    "    mus, ys = [], []\n",
    "    start_time = time.time()\n",
    "    total_batches = len(loader)\n",
    "\n",
    "    print(f\"\\n[INFO] Extracting features from {name} ({total_batches} batches)...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loader, start=1):\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            mu, _ = cvae.encode(x)\n",
    "            mus.append(mu.detach().cpu().numpy())\n",
    "            ys.append(y.numpy())\n",
    "\n",
    "            if i % 20 == 0 or i == total_batches:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"  Batch {i:>4}/{total_batches} | Time elapsed: {elapsed:5.1f}s\")\n",
    "\n",
    "    X = np.concatenate(mus, axis=0)\n",
    "    y = np.concatenate(ys, axis=0)\n",
    "    print(f\"[INFO] Done {name}: {X.shape[0]} samples, {X.shape[1]}-dim μs | Total time: {time.time()-start_time:.1f}s\\n\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# 1) Extract μ for all splits\n",
    "X_train, y_train = collect_mu(cvae, train_loader, device, \"train set\")\n",
    "X_valid, y_valid = collect_mu(cvae, valid_loader, device, \"validation set\")\n",
    "X_test, y_test = collect_mu(cvae, test_loader, device, \"test set\")\n",
    "\n",
    "# 2) Standardize\n",
    "print(\"[INFO] Standardizing features...\")\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_valid_s = scaler.transform(X_valid)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "print(\"  Done scaling.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8701fa63",
   "metadata": {},
   "source": [
    "# Train classifier/regressor \n",
    "Using the latent space representation of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba60e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9218cbb4",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05779f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(\n",
    "    clf, # scikit-learn classifier or pipeline\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    name: str=\"Model\")->None:\n",
    "    \n",
    "    y_predicted = clf.predict(X)\n",
    "    print(f\"[{name}] Accuracy:\", accuracy_score(y, y_predicted))\n",
    "    print(classification_report(y, y_predicted, digits=4))\n",
    "    return None\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(model, X_valid, y_valid, class_names=None,\n",
    "                          title=\"Confusion Matrix\", normalize=True, figsize=(7,6)):\n",
    "    \"\"\"\n",
    "    Compute and plot the confusion matrix for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : trained classifier (e.g., SVM, MLP)\n",
    "        Must implement `.predict()`.\n",
    "    X_valid : array-like\n",
    "        Validation features.\n",
    "    y_valid : array-like\n",
    "        True labels.\n",
    "    class_names : list of str, optional\n",
    "        Names of classes (e.g., CIFAR-10 class names). If None, uses label integers.\n",
    "    title : str, optional\n",
    "        Plot title.\n",
    "    normalize : bool, optional\n",
    "        Normalize each row to sum to 1.\n",
    "    figsize : tuple, optional\n",
    "        Figure size.\n",
    "    \"\"\"\n",
    "    # --- Predict ---\n",
    "    y_pred = model.predict(X_valid)\n",
    "\n",
    "    # --- Compute confusion matrix ---\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "    # --- Normalize if requested ---\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # --- Define labels for axes ---\n",
    "    if class_names is None:\n",
    "        labels = np.unique(y_valid)\n",
    "    else:\n",
    "        labels = class_names\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\".2f\" if normalize else \"d\", cmap=\"Blues\",\n",
    "        xticklabels=labels, yticklabels=labels\n",
    "    )\n",
    "    plt.title(title + (\" (Normalized)\" if normalize else \"\"))\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e1979",
   "metadata": {},
   "source": [
    "## Dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dummy baseline classifier ---\n",
    "# Strategy options:\n",
    "# \"most_frequent\" → always predicts the most common class\n",
    "# \"stratified\" → random predictions following class frequencies\n",
    "# \"uniform\" → random uniform over classes\n",
    "dummy = DummyClassifier(strategy=\"uniform\", random_state=42)\n",
    "dummy.fit(X_train_s, y_train)\n",
    "y_dummy = dummy.predict(X_valid_s)\n",
    "dummy_acc = dummy.score(X_valid_s, y_valid)\n",
    "\n",
    "print(f\"\\nBaseline (DummyClassifier, strategy='most_frequent') Accuracy: {dummy_acc:.4f}\\n\")\n",
    "print(\"Classification Report (Dummy Baseline):\")\n",
    "print(classification_report(y_valid, y_dummy, digits=4, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de90241e",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ffda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logistic Regression on UMAP features ---\n",
    "lr_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_clf.fit(X_train_s, y_train)\n",
    "y_pred = lr_clf.predict(X_valid_s)\n",
    "acc = lr_clf.score(X_valid_s, y_valid)\n",
    "\n",
    "print(f\"Validation Accuracy on UMAP(2D): {acc:.4f}\\n\")\n",
    "print(\"Classification Report (Logistic Regression):\")\n",
    "print(classification_report(y_valid, y_pred, digits=4))\n",
    "\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    lr_clf,\n",
    "    X_valid_s,\n",
    "    y_valid,\n",
    "    class_names=cifar10_classes,\n",
    "    title=\"SVM Classifier on CIFAR-10 (VAE Features)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3227e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logistic Regression on UMAP features ---\n",
    "# lr_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# lr_clf.fit(X_train_s, y_train)\n",
    "y_pred = lr_clf.predict(X_test_s)\n",
    "acc = lr_clf.score(X_test_s, y_test)\n",
    "\n",
    "print(f\"Test Accuracy on UMAP(2D): {acc:.4f}\\n\")\n",
    "print(\"Classification Report (Logistic Regression):\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    lr_clf,\n",
    "    X_test_s,\n",
    "    y_test,\n",
    "    class_names=cifar10_classes,\n",
    "    title=\"Test LR Classifier on CIFAR-10 (VAE Features)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7492b2",
   "metadata": {},
   "source": [
    "## Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eafa059",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf: Pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LinearSVC(dual=False, C=1.0, max_iter=5000, random_state=42)\n",
    ")\n",
    "\n",
    "svm_clf.fit(X_train_s, y_train)\n",
    "\n",
    "# Evaluate on valid features\n",
    "evaluate_classifier(clf=svm_clf, \n",
    "                    X=X_valid_s, \n",
    "                    y=y_valid, \n",
    "                    name=\"SVM\")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    svm_clf,\n",
    "    X_valid_s,\n",
    "    y_valid,\n",
    "    class_names=cifar10_classes,\n",
    "    title=\"SVM Classifier on CIFAR-10 (VAE Features)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9683ca77",
   "metadata": {},
   "source": [
    "## MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82bb1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PyTorch MLP with Train / Val / Test + Accuracy Plot + Regularization (Fixed) =====\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------- Setup -------------------------\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[INFO] Device:\", device)\n",
    "\n",
    "# ------------------------- Data (expects X_train_s, etc.) -------------------------\n",
    "X_train = torch.tensor(X_train_s, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "X_valid = torch.tensor(X_valid_s, dtype=torch.float32)\n",
    "y_valid_t = torch.tensor(y_valid, dtype=torch.long)\n",
    "X_test  = torch.tensor(X_test_s,  dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "# Standardize using train stats\n",
    "mu  = X_train.mean(0, keepdim=True)\n",
    "std = X_train.std(0, keepdim=True).clamp_min(1e-6)\n",
    "X_train = (X_train - mu) / std\n",
    "X_valid = (X_valid - mu) / std\n",
    "X_test  = (X_test  - mu) / std\n",
    "\n",
    "X_train = X_train.to(device); y_train_t = y_train_t.to(device)\n",
    "X_valid = X_valid.to(device); y_valid_t = y_valid_t.to(device)\n",
    "X_test  = X_test.to(device);  y_test_t  = y_test_t.to(device)\n",
    "\n",
    "# ------------------------- Hyperparams -------------------------\n",
    "dropout_p    = 0.2\n",
    "weight_decay = 1e-4\n",
    "lr           = 1e-4\n",
    "batch_size   = 256\n",
    "epochs       = 30\n",
    "use_early_stopping = True\n",
    "patience     = 5\n",
    "\n",
    "# ------------------------- Model (Fixed Architecture) -------------------------\n",
    "in_dim = X_train.shape[1]\n",
    "n_classes = int(y_train_t.max().item() + 1)\n",
    "\n",
    "print(f\"[INFO] Input dim = {in_dim}, Num classes = {n_classes}\")\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_dim, 128), nn.ReLU(),\n",
    "    nn.Dropout(dropout_p),\n",
    "    nn.Linear(128, 64),     nn.ReLU(),\n",
    "    nn.Dropout(dropout_p),\n",
    "    nn.Linear(64, n_classes)\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# ------------------------- Train -------------------------\n",
    "print(\"[INFO] Training MLP...\")\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []\n",
    "\n",
    "best_val_acc = -1.0\n",
    "best_state = None\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    perm = torch.randperm(X_train.size(0), device=device)\n",
    "    total_loss = 0.0\n",
    "    correct_train = 0\n",
    "\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        idx = perm[i:i+batch_size]\n",
    "        xb, yb = X_train[idx], y_train_t[idx]\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        correct_train += (logits.argmax(1) == yb).sum().item()\n",
    "\n",
    "    train_loss = total_loss / X_train.size(0)\n",
    "    train_acc = correct_train / X_train.size(0)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_val = model(X_valid)\n",
    "        val_loss = loss_fn(logits_val, y_valid_t).item()\n",
    "        preds_val = logits_val.argmax(1)\n",
    "        val_acc = (preds_val == y_valid_t).float().mean().item()\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} \"\n",
    "          f\"| train_acc={train_acc:.4f} | val_acc={val_acc:.4f}\")\n",
    "\n",
    "    # early stopping\n",
    "    if use_early_stopping:\n",
    "        if val_acc > best_val_acc + 1e-6:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"[INFO] Early stopping at epoch {epoch} (best val_acc={best_val_acc:.4f}).\")\n",
    "                break\n",
    "\n",
    "if use_early_stopping and best_state is not None:\n",
    "    model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "\n",
    "# ------------------------- Evaluation -------------------------\n",
    "def evaluate_split(X_tensor, y_np, name=\"Split\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_tensor).argmax(1).cpu().numpy()\n",
    "    acc = accuracy_score(y_np, y_pred)\n",
    "    print(f\"\\n[{name} Accuracy]: {acc:.4f}\")\n",
    "    print(f\"Classification Report ({name}):\")\n",
    "    print(classification_report(y_np, y_pred, digits=4))\n",
    "    return y_pred, acc\n",
    "\n",
    "y_pred_valid, valid_acc = evaluate_split(X_valid, y_valid, name=\"Validation\")\n",
    "y_pred_test, test_acc   = evaluate_split(X_test,  y_test,  name=\"Test\")\n",
    "\n",
    "# ------------------------- Plots -------------------------\n",
    "epochs_axis = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(epochs_axis, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs_axis, val_losses, label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss vs Epoch\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(epochs_axis, train_accs, label=\"Train Accuracy\")\n",
    "plt.plot(epochs_axis, val_accs, label=\"Val Accuracy\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Train & Validation Accuracy vs Epoch\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(); plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
