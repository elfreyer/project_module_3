{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00cd7439",
   "metadata": {},
   "source": [
    "# Cifar10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5796909",
   "metadata": {},
   "source": [
    "# Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d98aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b0a1f7",
   "metadata": {},
   "source": [
    "# Setup\n",
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5175626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "\n",
    "# TODO: investigate the rescalling option\n",
    "# IMG_SIZE = 160 # origninal img size (32, 32, 3), optimal img size (224, 224, 3)\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform: Compose = Compose([\n",
    "    \n",
    "    # Resize((32, 32)),\n",
    "    # Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] \n",
    "    # to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "    ToTensor(),\n",
    "    \n",
    "    # Normalize a tensor image with mean and standard deviation.\n",
    "    # output[channel] = (input[channel] - mean[channel]) / std[channel] -> normalised = (original - 0.5) / 0.5]\n",
    "    # Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be58c021",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ce6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# load the full CIFAR-10 training data\n",
    "full_trainset: CIFAR10 = CIFAR10(\n",
    "    root='../data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# split the full training data into train and validation sets\n",
    "train_size: int = int(0.8 * len(full_trainset))\n",
    "valid_size: int = len(full_trainset) - train_size\n",
    "\n",
    "# TODO: check annotation type, and understand the function\n",
    "trainset, validset = random_split(full_trainset, [train_size, valid_size])\n",
    "\n",
    "# load the test data\n",
    "testset: CIFAR10 = CIFAR10(\n",
    "    root='../data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bd9cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataset sizes\n",
    "print(f\"All samples: {len(trainset)+len(validset)+len(testset)}\")\n",
    "print(f\"Train samples: {len(trainset)}\")\n",
    "print(f\"Validation samples: {len(validset)}\")\n",
    "print(f\"Test samples: {len(testset)}\")\n",
    "\n",
    "# get one sample to check image shape\n",
    "image, label = full_trainset[0]\n",
    "print(f\"Single image shape: {image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f193b28",
   "metadata": {},
   "source": [
    "## Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8741d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader: DataLoader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4,    \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "# \n",
    "# validloader = torch.utils.data.DataLoader(\n",
    "valid_loader: DataLoader = DataLoader(\n",
    "    validset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=4,    \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_loader: DataLoader = DataLoader(\n",
    "    testset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=4,    \n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# Check shapes\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"Batch shape:\", xb.shape)   # should be [B, 3, 32, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971a1eca",
   "metadata": {},
   "source": [
    "## View a batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e39f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# TODO: check the bug -> it is not an 8 by 8 figure, why?\n",
    "# Helper function to display an image grid\n",
    "def show_image(image_tensor: torch.Tensor) -> None:\n",
    "    \"\"\"Convert a PyTorch tensor into a viewable image grid and show it.\"\"\"\n",
    "\n",
    "    # Step 1 Undo normalization: normalised = (original - 0.5) / 0.5] -> normalised * 2 +0.5 = original\n",
    "    unnormalized_image_grid = image_tensor * 0.5 + 0.5\n",
    "\n",
    "    # Step 2: Convert from a PyTorch tensor to a NumPy array (for matplotlib)\n",
    "    # .detach() removes gradient tracking; .cpu() ensures data is on the CPU.\n",
    "    image_grid_C_H_W = unnormalized_image_grid.detach().cpu().numpy()\n",
    "    \n",
    "    print(\"Shape before transpose:\", image_grid_C_H_W.shape)  \n",
    "    # (C, H, W) → C = number of color channels (3 for RGB), H = height, W = width\n",
    "\n",
    "    # Step 3: Rearrange dimensions from (C, H, W) to (H, W, C)\n",
    "    # because Matplotlib expects the color channel as the last dimension.\n",
    "    image_grid_H_W_C = np.transpose(image_grid_C_H_W, (1, 2, 0))\n",
    "    print(\"Shape after transpose:\", image_grid_H_W_C.shape)   # (H, W, C)\n",
    "\n",
    "    # Step 4: Display the image\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(image_grid_H_W_C)\n",
    "    plt.axis(\"off\")  # hide axis numbers\n",
    "    plt.show()\n",
    "    \n",
    "# Get a batch of training images\n",
    "data_iter: iter = iter(train_loader)  # Create an iterator for the DataLoader\n",
    "images: torch.Tensor\n",
    "labels: torch.Tensor\n",
    "images, labels = next(data_iter)   # Get one batch (images + labels)\n",
    "\n",
    "# Make a grid from the batch\n",
    "image_grid: torch.Tensor = torchvision.utils.make_grid(images, nrow=8)  # 8 images per row\n",
    "\n",
    "\n",
    "# Show the grid\n",
    "show_image(image_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3da908",
   "metadata": {},
   "source": [
    "## Get class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fcf06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get the label directly cifar10_classes = cifar10.classes\n",
    "\n",
    "cifar10_classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(cifar10_classes [labels[0]])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6889a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7538161a",
   "metadata": {},
   "source": [
    "# Convolutional Variational Autoencoder (Conv-VAE)\n",
    "https://github.com/ageron/handson-mlp/blob/main/18_autoencoders_gans_and_diffusion_models.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a46f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d30d8",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a175176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60463daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAEOutput = namedtuple(\"VAEOutput\", [\"output\", \"codings_mean\", \"codings_logvar\"])\n",
    "# TODO: rename VAE model to CVAE\n",
    "# TODO: try batchnorm, dropout\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, codings_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        self.codings_dim = codings_dim\n",
    "\n",
    "        # Encoder: 3x32x32 -> 256x4x4\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),   # 32x32 -> 16x16\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 16x16 -> 8x8\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1), # 8x8 -> 4x4\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),# 4x4 -> 4x4\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.flat_dim = 256 * 4 * 4  # 4096 <-- for image 32x32\n",
    "\n",
    "        # Variational heads\n",
    "        self.fc_mu     = nn.Linear(self.flat_dim, codings_dim)\n",
    "        self.fc_logvar = nn.Linear(self.flat_dim, codings_dim)\n",
    "\n",
    "        # Decoder: z -> 256x4x4 -> 3x32x32\n",
    "        self.fc_dec = nn.Linear(codings_dim, self.flat_dim)\n",
    "        self.decoder_cnn = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 4->8\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 8->16\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),    # 16->32\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1),              # logits or probs (see note)\n",
    "            nn.Sigmoid()  # <- keep this since your loss uses MSE on [0,1]\n",
    "        )\n",
    "\n",
    "    def encode(self, X):\n",
    "        h = self.encoder_cnn(X).flatten(1)              # (B, 4096)\n",
    "        mu = self.fc_mu(h)                               # (B, codings_dim)\n",
    "        logvar = self.fc_logvar(h)                       # (B, codings_dim)\n",
    "        return mu, logvar\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_codings(mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def decode(self, Z):\n",
    "        h = self.fc_dec(Z).view(-1, 256, 4, 4)          # (B,256,4,4) \n",
    "        x_recon = self.decoder_cnn(h)                   # (B,3,32,32) in [0,1]\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean, logvar = self.encode(X)\n",
    "        Z = self.sample_codings(mean, logvar)\n",
    "        output = self.decode(Z)\n",
    "        return VAEOutput(output, mean, logvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6e25a",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(y_pred, x_target, kl_weight=1.0):\n",
    "    output, mean, logvar = y_pred\n",
    "    # MSE reconstruction\n",
    "    recon = F.mse_loss(output, x_target)\n",
    "    # recon = F.binary_cross_entropy_with_logits(output, x_target, reduction=\"mean\") # logits + BCEWithLogitsLoss:\n",
    "    # KL divergence\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - logvar.exp() - mean.pow(2), dim=-1).mean()\n",
    "    # Scale KL to per-pixel magnitude\n",
    "    n_pixels = x_target[0].numel()\n",
    "    return recon + kl_weight * (kl_div / n_pixels)\n",
    "\n",
    "def evaluate_tm(model, data_loader, metric):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            out = y_pred.output if isinstance(y_pred, tuple) else y_pred\n",
    "            metric.update(out, X_batch)  # compare recon to input\n",
    "    return metric.compute()\n",
    "\n",
    "def train(model, optimizer, loss_fn, metric, train_loader, valid_loader, n_epochs=20):\n",
    "    history = {\"train_losses\": [], \"train_metrics\": [], \"valid_metrics\": []}\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.0\n",
    "        metric.reset()\n",
    "        model.train()\n",
    "        for index, (X_batch, _) in enumerate(train_loader):\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, X_batch)   # use input as target\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = y_pred.output if isinstance(y_pred, tuple) else y_pred\n",
    "            metric.update(out, X_batch)\n",
    "\n",
    "        train_metric = metric.compute().item()\n",
    "        val_metric = evaluate_tm(model, valid_loader, metric).item()\n",
    "\n",
    "        history[\"train_losses\"].append(total_loss / len(train_loader))\n",
    "        history[\"train_metrics\"].append(train_metric)\n",
    "        history[\"valid_metrics\"].append(val_metric)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d}/{n_epochs} \"\n",
    "              f\"loss={history['train_losses'][-1]:.4f}, \"\n",
    "              f\"train RMSE={train_metric:.4f}, val RMSE={val_metric:.4f}\")\n",
    "    return history\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "vae = VAE(codings_dim=32).to(device)\n",
    "# TODO: hyperparameter to tune -> learning rate\n",
    "optimizer = torch.optim.NAdam(vae.parameters(), lr=1e-4)\n",
    "rmse = torchmetrics.MeanSquaredError(squared=False).to(device)\n",
    "\n",
    "history = train(vae, optimizer, vae_loss, rmse, train_loader, valid_loader, n_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d82386",
   "metadata": {},
   "source": [
    "## Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdab9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Plot training loss ---\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history[\"train_losses\"], label=\"Train loss\", color=\"tab:blue\")\n",
    "plt.title(\"VAE Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot RMSE (train vs validation) ---\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history[\"train_metrics\"], label=\"Train RMSE\", color=\"tab:green\")\n",
    "plt.plot(history[\"valid_metrics\"], label=\"Validation RMSE\", color=\"tab:orange\")\n",
    "plt.title(\"Reconstruction RMSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6f46c9",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927b69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def plot_image(img):\n",
    "    \"\"\"Convert a tensor to a displayable image and plot it.\"\"\"\n",
    "    img = img.detach().cpu()\n",
    "    if img.ndim == 3:\n",
    "        img = img.permute(1, 2, 0)  # (C,H,W) -> (H,W,C)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def plot_reconstructions(model, data_loader, n_images=8):\n",
    "    \"\"\"\n",
    "    Show original (top row) and reconstructed (bottom row) images from the VAE.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # --- Get a batch of images ---\n",
    "    X_batch, _ = next(iter(data_loader))\n",
    "    X_batch = X_batch.to(device)\n",
    "    X_batch = X_batch[:n_images]  # take first n_images only\n",
    "\n",
    "    # --- Forward pass ---\n",
    "    with torch.no_grad():\n",
    "        out = model(X_batch)\n",
    "        recon = out.output if hasattr(out, \"output\") else out\n",
    "\n",
    "    # --- Plot originals and reconstructions ---\n",
    "    fig, axes = plt.subplots(2, n_images, figsize=(n_images * 1.5, 3))\n",
    "\n",
    "    for i in range(n_images):\n",
    "        # Top row: original\n",
    "        axes[0, i].imshow(X_batch[i].detach().cpu().permute(1, 2, 0))\n",
    "        axes[0, i].axis(\"off\")\n",
    "\n",
    "        # Bottom row: reconstruction\n",
    "        axes[1, i].imshow(recon[i].detach().cpu().permute(1, 2, 0))\n",
    "        axes[1, i].axis(\"off\")\n",
    "    \n",
    "\n",
    "    fig.suptitle(\"Top: Original images | Bottom: Reconstructions\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Use it ---\n",
    "plot_reconstructions(vae, valid_loader, n_images=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee3e58",
   "metadata": {},
   "source": [
    "# Exploration of the latent represenation (umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def collect_mu(vae, loader, device):\n",
    "    vae.eval()\n",
    "    mus, ys = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            out = vae(x)\n",
    "            # Handle both: namedtuple VAEOutput or plain tensor\n",
    "            mu = out.codings_mean if hasattr(out, \"codings_mean\") else vae.encode(x)[0]\n",
    "            mus.append(mu.detach().cpu().numpy())\n",
    "            ys.append(y.numpy())\n",
    "    X_mu = np.concatenate(mus, axis=0)   # shape: (N, codings_dim)\n",
    "    y_all = np.concatenate(ys, axis=0)   # shape: (N,)\n",
    "    return X_mu, y_all\n",
    "\n",
    "# Example:\n",
    "X_train_mu, y_train = collect_mu(vae, train_loader, device)\n",
    "X_valid_mu, y_valid = collect_mu(vae, valid_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bee2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install umap-learn\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# TODO: try to improve by playing with the parameters\n",
    "\n",
    "umap_2d = umap.UMAP(\n",
    "    n_neighbors=15,      # local vs global structure (try 10–50)\n",
    "    min_dist=0.1,        # how tight clusters look (0.0–0.5)\n",
    "    n_components=2,      # 2D for plotting (set 3 for 3D)\n",
    "    metric=\"euclidean\",  # try \"cosine\" if features are directional\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Z_train_2d = umap_2d.fit_transform(X_train_mu)  # (N_train, 2)\n",
    "# Z_valid_2d = umap_2d.transform(X_valid_mu)      # (N_valid, 2)\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler().fit(X_train_mu)\n",
    "X_train_mu_s = scaler.transform(X_train_mu)\n",
    "X_valid_mu_s = scaler.transform(X_valid_mu)\n",
    "Z_train_2d = umap_2d.fit_transform(X_train_mu_s)\n",
    "Z_valid_2d = umap_2d.transform(X_valid_mu_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a4295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scatter_umap(Z, y, title=\"UMAP of VAE μ\", num_classes=None):\n",
    "    plt.figure(figsize=(7,6))\n",
    "    sc = plt.scatter(Z[:,0], Z[:,1], c=y, s=8, alpha=0.8, cmap=\"tab10\")\n",
    "    if num_classes is None:\n",
    "        num_classes = len(np.unique(y))\n",
    "    plt.colorbar(sc, ticks=range(num_classes))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"UMAP-1\"); plt.ylabel(\"UMAP-2\")\n",
    "    plt.grid(True, ls=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "scatter_umap(Z_train_2d, y_train, title=\"Train μ → UMAP (2D)\")\n",
    "scatter_umap(Z_valid_2d, y_valid, title=\"Valid μ → UMAP (2D)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827f9ecb",
   "metadata": {},
   "source": [
    "Overall structure\n",
    "\n",
    "* The map shows a continuous, dense cloud rather than distinct clusters.\n",
    "\n",
    "* That’s expected — your autoencoder is trained unsupervised only to reconstruct images, not to separate classes.\n",
    "\n",
    "* Therefore, latent codes mainly capture visual similarity (colors, textures, shapes) rather than semantic categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc0ec2b",
   "metadata": {},
   "source": [
    "# Find outliers\n",
    "test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28753ba",
   "metadata": {},
   "source": [
    "# Sample from the latent distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef77f5",
   "metadata": {},
   "source": [
    "# overlay with data points\n",
    "What does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038db47b",
   "metadata": {},
   "source": [
    "# Feature extraction \n",
    "From latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee655b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(vae.parameters()).device\n",
    "\n",
    "# --- helper to collect μ and labels from a loader ---\n",
    "def collect_mu(vae, loader, device, name=\"dataset\"):\n",
    "    vae.eval()\n",
    "    mus, ys = [], []\n",
    "    start_time = time.time()\n",
    "    total_batches = len(loader)\n",
    "\n",
    "    print(f\"\\n[INFO] Extracting features from {name} ({total_batches} batches)...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loader, start=1):\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            mu, _ = vae.encode(x)\n",
    "            mus.append(mu.detach().cpu().numpy())\n",
    "            ys.append(y.numpy())\n",
    "\n",
    "            if i % 20 == 0 or i == total_batches:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"  Batch {i:>4}/{total_batches} | Time elapsed: {elapsed:5.1f}s\")\n",
    "\n",
    "    X = np.concatenate(mus, axis=0)\n",
    "    y = np.concatenate(ys, axis=0)\n",
    "    print(f\"[INFO] Done {name}: {X.shape[0]} samples, {X.shape[1]}-dim μs | Total time: {time.time()-start_time:.1f}s\\n\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# 1) Extract μ for all splits\n",
    "X_train, y_train = collect_mu(vae, train_loader, device, \"train set\")\n",
    "X_valid, y_valid = collect_mu(vae, valid_loader, device, \"validation set\")\n",
    "\n",
    "# 2) Standardize\n",
    "print(\"[INFO] Standardizing features...\")\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_valid_s = scaler.transform(X_valid)\n",
    "print(\"  Done scaling.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8701fa63",
   "metadata": {},
   "source": [
    "# Train classifier/regressor \n",
    "Using the latent space representation of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba60e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import time\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9218cbb4",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05779f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(\n",
    "    clf, # scikit-learn classifier or pipeline\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    name: str=\"Model\")->None:\n",
    "    \n",
    "    y_predicted = clf.predict(X)\n",
    "    print(f\"[{name}] Accuracy:\", accuracy_score(y, y_predicted))\n",
    "    print(classification_report(y, y_predicted, digits=4))\n",
    "    return None\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(model, X_valid, y_valid, class_names=None,\n",
    "                          title=\"Confusion Matrix\", normalize=True, figsize=(7,6)):\n",
    "    \"\"\"\n",
    "    Compute and plot the confusion matrix for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : trained classifier (e.g., SVM, MLP)\n",
    "        Must implement `.predict()`.\n",
    "    X_valid : array-like\n",
    "        Validation features.\n",
    "    y_valid : array-like\n",
    "        True labels.\n",
    "    class_names : list of str, optional\n",
    "        Names of classes (e.g., CIFAR-10 class names). If None, uses label integers.\n",
    "    title : str, optional\n",
    "        Plot title.\n",
    "    normalize : bool, optional\n",
    "        Normalize each row to sum to 1.\n",
    "    figsize : tuple, optional\n",
    "        Figure size.\n",
    "    \"\"\"\n",
    "    # --- Predict ---\n",
    "    y_pred = model.predict(X_valid)\n",
    "\n",
    "    # --- Compute confusion matrix ---\n",
    "    cm = confusion_matrix(y_valid, y_pred)\n",
    "\n",
    "    # --- Normalize if requested ---\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # --- Define labels for axes ---\n",
    "    if class_names is None:\n",
    "        labels = np.unique(y_valid)\n",
    "    else:\n",
    "        labels = class_names\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\".2f\" if normalize else \"d\", cmap=\"Blues\",\n",
    "        xticklabels=labels, yticklabels=labels\n",
    "    )\n",
    "    plt.title(title + (\" (Normalized)\" if normalize else \"\"))\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235e6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dummy baseline classifier ---\n",
    "# Strategy options:\n",
    "# \"most_frequent\" → always predicts the most common class\n",
    "# \"stratified\" → random predictions following class frequencies\n",
    "# \"uniform\" → random uniform over classes\n",
    "dummy = DummyClassifier(strategy=\"uniform\", random_state=42)\n",
    "dummy.fit(X_train_s, y_train)\n",
    "y_dummy = dummy.predict(X_valid_s)\n",
    "dummy_acc = dummy.score(X_valid_s, y_valid)\n",
    "\n",
    "print(f\"\\nBaseline (DummyClassifier, strategy='most_frequent') Accuracy: {dummy_acc:.4f}\\n\")\n",
    "print(\"Classification Report (Dummy Baseline):\")\n",
    "print(classification_report(y_valid, y_dummy, digits=4, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ffda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Logistic Regression on UMAP features ---\n",
    "lr_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_clf.fit(X_train_s, y_train)\n",
    "y_pred = lr_clf.predict(X_valid_s)\n",
    "acc = lr_clf.score(X_valid_s, y_valid)\n",
    "\n",
    "print(f\"Validation Accuracy on UMAP(2D): {acc:.4f}\\n\")\n",
    "print(\"Classification Report (Logistic Regression):\")\n",
    "print(classification_report(y_valid, y_pred, digits=4))\n",
    "\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    lr_clf,\n",
    "    X_valid_s,\n",
    "    y_valid,\n",
    "    class_names=cifar10_classes,\n",
    "    title=\"SVM Classifier on CIFAR-10 (VAE Features)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eafa059",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf: Pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LinearSVC(dual=False, C=1.0, max_iter=5000, random_state=42)\n",
    ")\n",
    "\n",
    "svm_clf.fit(X_train_s, y_train)\n",
    "\n",
    "# Evaluate on valid features\n",
    "evaluate_classifier(clf=svm_clf, \n",
    "                    X=X_valid_s, \n",
    "                    y=y_valid, \n",
    "                    name=\"SVM\")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    svm_clf,\n",
    "    X_valid_s,\n",
    "    y_valid,\n",
    "    class_names=cifar10_classes,\n",
    "    title=\"SVM Classifier on CIFAR-10 (VAE Features)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# device = next(vae.parameters()).device\n",
    "\n",
    "# # --- helper to collect μ and labels from a loader ---\n",
    "# def collect_mu(vae, loader, device, name=\"dataset\"):\n",
    "#     vae.eval()\n",
    "#     mus, ys = [], []\n",
    "#     start_time = time.time()\n",
    "#     total_batches = len(loader)\n",
    "\n",
    "#     print(f\"\\n[INFO] Extracting features from {name} ({total_batches} batches)...\")\n",
    "#     with torch.no_grad():\n",
    "#         for i, (x, y) in enumerate(loader, start=1):\n",
    "#             x = x.to(device, non_blocking=True)\n",
    "#             mu, _ = vae.encode(x)\n",
    "#             mus.append(mu.detach().cpu().numpy())\n",
    "#             ys.append(y.numpy())\n",
    "\n",
    "#             if i % 20 == 0 or i == total_batches:\n",
    "#                 elapsed = time.time() - start_time\n",
    "#                 print(f\"  Batch {i:>4}/{total_batches} | Time elapsed: {elapsed:5.1f}s\")\n",
    "\n",
    "#     X = np.concatenate(mus, axis=0)\n",
    "#     y = np.concatenate(ys, axis=0)\n",
    "#     print(f\"[INFO] Done {name}: {X.shape[0]} samples, {X.shape[1]}-dim μs | Total time: {time.time()-start_time:.1f}s\\n\")\n",
    "#     return X, y\n",
    "\n",
    "\n",
    "# # 1) Extract μ for all splits\n",
    "# X_train, y_train = collect_mu(vae, train_loader, device, \"train set\")\n",
    "# X_valid, y_valid = collect_mu(vae, valid_loader, device, \"validation set\")\n",
    "\n",
    "# # 2) Standardize\n",
    "# print(\"[INFO] Standardizing features...\")\n",
    "# scaler = StandardScaler().fit(X_train)\n",
    "# X_train_s = scaler.transform(X_train)\n",
    "# X_valid_s = scaler.transform(X_valid)\n",
    "# print(\"  Done scaling.\\n\")\n",
    "\n",
    "# # 3) Train the MLP\n",
    "# print(\"[INFO] Training MLP classifier on latent μ features...\")\n",
    "# start = time.time()\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(256,), max_iter=100, alpha=1e-4, random_state=42)\n",
    "# clf.fit(X_train_s, y_train)\n",
    "# print(f\"  MLP training done in {time.time()-start:.1f}s\\n\")\n",
    "\n",
    "# # 4) Evaluate\n",
    "# print(\"[INFO] Evaluating on validation set...\")\n",
    "# y_pred_val = clf.predict(X_valid_s)\n",
    "# val_acc = accuracy_score(y_valid, y_pred_val)\n",
    "# print(f\"\\nValidation accuracy (μ → MLP): {val_acc:.4f}\\n\")\n",
    "\n",
    "# print(\"Classification Report (Validation):\")\n",
    "# print(classification_report(y_valid, y_pred_val, digits=4, zero_division=0))\n",
    "\n",
    "# cm = confusion_matrix(y_valid, y_pred_val)\n",
    "# print(\"Confusion Matrix (Validation):\")\n",
    "# print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
